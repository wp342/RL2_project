@article{terry2020pettingzoo,
  Title = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
  Author = {Terry, J. K and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sulivan, Ryan and Santos, Luis and Perez, Rodrigo and Horsch, Caroline and Dieffendahl, Clemens and Williams, Niall L and Lokesh, Yashas and Sullivan, Ryan and Ravi, Praveen},
  journal={arXiv preprint arXiv:2009.14471},
  year={2020}
}

@INPROCEEDINGS{9263738,
  author={Paczolay, Gabor and Harmati, Istvan},
  booktitle={2020 23rd International Symposium on Measurement and Control in Robotics (ISMCR)},
  title={A New Advantage Actor-Critic Algorithm For Multi-Agent Environments},
  year={2020},
  volume={},
  umber={},
  pages={1-6},
  doi={10.1109/ISMCR51255.2020.9263738}
}

@misc{PPOAlgo,
  doi = {10.48550/ARXIV.1707.06347},
  url = {https://arxiv.org/abs/1707.06347},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Proximal Policy Optimization Algorithms},
  publisher = {arXiv},
  year = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{PPOmultiagentgames,
  doi = {10.48550/ARXIV.2103.01955},
  url = {https://arxiv.org/abs/2103.01955},
  author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{AdvanacesinNIPS,
 author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
 url = {https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf},
 volume = {12},
 year = {1999}
}

@inproceedings{armsrace,
  author          = {Dawkins, Richard and Krebs, John Richard},
  doi             = {10.1098/rspb.1979.0081},
  booktitle       = {Proceedings of the Royal Society of London. Series B. Biological Sciences. Royal Society, 205(1161),},
  title           = {Arms races between and within species},
  year            = {1979},
  pages           = {489-511},
}

@misc{emergenttoolusage,
  doi = {10.48550/ARXIV.1909.07528},
  url = {https://arxiv.org/abs/1909.07528},
  author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Multiagent Systems (cs.MA), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Emergent Tool Use From Multi-Agent Autocurricula},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{trustregionpolicy,
  doi = {10.48550/ARXIV.1502.05477},
  url = {https://arxiv.org/abs/1502.05477},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Trust Region Policy Optimization},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{Mnih2015,
author={Mnih, Volodymyr
and Kavukcuoglu, Koray
and Silver, David
and Rusu, Andrei A.
and Veness, Joel
and Bellemare, Marc G.
and Graves, Alex
and Riedmiller, Martin
and Fidjeland, Andreas K.
and Ostrovski, Georg
and Petersen, Stig
and Beattie, Charles
and Sadik, Amir
and Antonoglou, Ioannis
and King, Helen
and Kumaran, Dharshan
and Wierstra, Daan
and Legg, Shane
and Hassabis, Demis},
title={Human-level control through deep reinforcement learning},
journal={Nature},
year={2015},
month={Feb},
day={01},
volume={518},
number={7540},
pages={529-533},
abstract={An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
issn={1476-4687},
doi={10.1038/nature14236},
url={https://doi.org/10.1038/nature14236}
}

ï»¿@Article{Williams1992,
author={Williams, Ronald J.},
title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
journal={Machine Learning},
year={1992},
month={May},
day={01},
volume={8},
number={3},
pages={229-256},
abstract={This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
issn={1573-0565},
doi={10.1007/BF00992696},
url={https://doi.org/10.1007/BF00992696}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={https://www.tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{adam-optimiser,
title={ {TensorFlow} Adam Optimiser},
url={https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam},
note={Tensorflow's Adam Optimiser},
author={
    Mart\'{i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dandelion~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{REINFORCE,
  url = {https://www.analyticsvidhya.com/blog/2020/11/reinforce-algorithm-taking-baby-steps-in-reinforcement-learning/},
  author = {Noufal, S},
  title = {REINFORCE Algorithm: Taking baby steps in reinforcement learning},
  publisher = {Analytics Vidhya},
  year = {2020},
  note = {Accessed: 3rd May 2022},
}

@article{mordatch2017emergence,
  title={Emergence of Grounded Compositional Language in Multi-Agent Populations},
  author={Mordatch, Igor and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1703.04908},
  year={2017}
}