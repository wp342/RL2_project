\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading rl_project.

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{rl_project}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{rl_project}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{amsmath}
\captionsetup[table]{skip=2.5pt}

\graphicspath{{images/}}



\DeclareCaptionType{equ}[][]
% Give your project report an appropriate title!

\title{RL Project: Investigation into the use of Deep Q Learning and PPO in a multiagent environment}


% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Steven Etches \\
  \texttt{spe24@bath.ac.uk} \\
  \And
  Mark Hazell \\
  \texttt{mph55@bath.ac.uk} \\
  \And
  Adam Rasool \\
  \texttt{aar73@bath.ac.uk} \\
  \And
  Will Prior \\
  \texttt{wp342@bath.ac.uk}
}

\begin{document}



\maketitle

\section{Problem Definition} \label{Problem Definition}

The reinforcement learning problem that we have chosen to address is from the multiagent reinforcement learning (MARL) environments provided in PettingZoo.ml by \citet{terry2020pettingzoo}.
These environments represent a generalisation of a predator and prey type environment that has good agents and adversaries, with some of the more complex environments also incorporating landmarks and objects.
The environment that has been chosen is the Simple World Comm scenario \citet{mordatch2017emergence}.

Simple World Comm consists of six agents, two on the "good" team and four on the "adversary" team.
Good agents are rewarded for proximity to a 'food' object(s), adversaries that are rewarded for collisions with good agents, and also there are two additional type of objects, a barrier that blocks the way and two 'shrubberies' that hide the position of the agents from observation.
One of the adversary agents is leader adversary that can always see the location of the good agents and communicate that location to the other adversary agents.

Good agents will attempt to maximise their reward by remaining proximal to the 'food' while also attempting to minimise the number of collisions with adversaries.
Adversaries on the other hand will be focused on attempting to collide and remain as close as possible with the good agents.

While the environment exists on an infinite plain, the good agents are incentivised to not move too far away from the origin point by an increasingly large negative reward every step once a threshold is crossed proportional to the distance from the origin.
Adversarial agents are not penalised in this manner but since they are already rewarded for attempting to collide with the good agents, if the good agents remain within the bounded area so should all the adversaries.


\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.25]{simple_comm_environment.png}
  \caption{Simple Comm World Environment}
  \label{fig:simple_comm_world}
\end{figure}


% \clearpage
The transition dynamics for the environment is a cycle as depicted in figure \ref{fig:stepcycle} found in Appendix A where each agent gets to take an action, once all agents have acted that step can be considered completed beginning a new step and cycle.
The default episode length for the environments is 25 steps however this was extended in this project to a minimum length of 50 depending on the algorithm used. 
This difference was primarily dictated by training time.

The adversary and good agents both share the same action of space of either taking no action or, moving in the cardinal directions.
The adversary leader's action space is the product of the movement actions that the other agents have and also the ability to either not communicate or communicate with one of the other adversary agents.
These actions spaces are detailed in tables \ref{tab:good-agent-action}, \ref{tab:adversary-agent-action}, and \ref{tab:adversary-leader-action} in Appendix A.

Good agents are rewarded for both colliding with the food objects and also maintaining a position within proximity of the food.
The reward for being proximal to the food decreasing with distance.
Good agents are punished with a negative reward colliding with adversaries.
Adversaries are rewarded for colliding with good agents while earning a punishment each step in the form of a negative reward proportional to how close good agents are to the food.
More details on the reward structure of the environment can be found in table \ref{tab:agent-rewards} in Appendix A

This project focuses on the application of the Deep Q Learning and Actor Critic algorithms to this environment, comparing their relative effectiveness to each other.
Of particular interest will be contrasting the rate at which the two algorithms learn and also the rewards that their optimal policy results in.

\section{Background} \label{Background}

The first reinforcement methods we have considered for this problem are:

\begin{itemize}
    \item Deep-Q Network
    \item REINFORCE
    \item Advantage Actor-Critic (A2C)
    \item Proximal Policy Optimisation (PPO)
\end{itemize}

The Deep-Q Network algorithm was first developed by Deep Mind in 2015 \citet{Mnih2015}.
This uses a combination of the reinforcement algorithm, Q-learning, and deep neural networks.
It has been implemented on many Atari games in the OpenAI gym and achieved levels far exceeding human capabilities \citet{Mnih2015}.

Algorithms that follow a value-function approach whilst performing well in some deterministic environments have been shown to be unable to converge to approximate a stochastic policy \citet{AdvanacesinNIPS}.
The REINFORCE algorithm is a type of policy gradient algorithm that provide unbiased estimates of gradient but without the learned value function and are able to make weight adjustments in the direction of gradient reinforcement \citet{Williams1992}.
Whilst this is appropriate for our stochastic environment, the main weakness is the long time required to train the agents. For example when implemented on Pong and Lunar Lander it reportedly took 96 hours when running on cloud GPU \citet{REINFORCE}.

A further group of policy gradient based algorithms which may be applied to this problem are proximal policy optimisation (PPO) algorithms as discussed by \citet{PPOAlgo}.
This family of algorithms avoids excessively large policy updates by considering a policy ratio between the new and old policies and ensuring that this ratio stays within an acceptable range $1\pm\varepsilon$ where $\varepsilon$ is a hyperparameter.
Owing to their relative simplicity in addition to their effectiveness \citet{PPOmultiagentgames}, these algorithms are popular, with research available which extends their application to multiagent systems such as that discussed in \citet{PPOAlgo} which outlines the multiagent PPO algorithm MAPPO.

In a like manner to the extension of the single-agent PPO algorithm to a multiagent problem cited above, \citet{9263738} propose a modification to the advantage actor-critic (A2C) algorithm.
The vanilla A2C algorithm utilises two neural networks, one to optimise on the basis of policy and a second to optimise value; respectively controlling agent behaviour and providing a measure of how 'good' the agent's actions are.
The modified algorithm proposed in this work applies this philosophy to a cooperative multiagent problem and produces very promising results in the author's testing.
It is noted however that “… the algorithm has the caveat of only being able to be used when the agents are fully cooperative without any special predefined roles between them”. \citet{terry2020pettingzoo}

\section{Method} \label{Method}

During the initial implementation of both of the Deep Q Learning and A2C algorithms a simpler problem was used allow for easier testing for correctness of the implementations.
The Simple Push environment is characterised by two particles, an adversary and an agent, which are attempting to earn as large a reward as possible for being close to a landmark.
The agent is rewarded proportional to its closeness to the landmark while the adversary is rewarded in two parts.
First the adversary receives a reward proportional to its closeness to the landmark and, second it is rewarded proportionally for how far away the agent is from the landmark.
This environment incentivises that adversary to both keep close to the landmark and also to keep the agent as far away from the landmark as possible.

\subsection{Deep Q-Learning} \label{DQN-Method}

In traditional Q-learning the algorithm records the value of state-action pairs in what is known as a Q-table.
Given a state the algorithm will first determine if it will explore randomly or taking an action which returns the maximum reward available (act greedily).
If the choice to act greedily is made then a lookup will be made to the Q-table and if there are state-action pair values recorded it will choose the one in accordance to an action-value function.

However, as the state and action spaces grow in both scale and complexity it becomes increasingly difficult and impractical to attempt to capture every state-action pair in a tabular form.
\citet{Mnih2015} instead proposed utilising a neural network to instead approximate the action-value function.
By presenting the state to a neural network and then utilising the outputs to approximate the action-value function makes it possible to apply a behavioural policy typical to traditional Q-learning algorithms to these more complex problems.

The high levels of correlation present in reinforcement learning environments (each step is directly correlated to both the one before and after) can pose a problem to neural nets where they are prone to over fitting.
To combat this experience replay was introduced where with each step a random batch of previous state, action, reward tuples is selected and used to train the net.
By utilising this batch method of training it reduces the correlation between the data to reduce the chance of overfitting.

Another issue that can effect deep Q-learning algorithms is that they can be unstable and prone to sudden changes if the error is too large when using a normal certain loss formulae such as squared error.
This tendency for large changes to be made to the network is mitigated by utilising a different loss formula such as the Huber Loss utilised in this project which less severely punishes large errors.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.3]{Huber_loss.png}
  \caption{Huber loss (green) and squared error loss (blue) as a function of y - f(x) \citep{huber_loss_dia}}
  \label{fig:huber-loss}
\end{figure}

A final technique to improve the accuracy of the neural net approximations is to use a fixed target network allowing for updates to be made towards a stationary target.
This target network's weights are updated from the primary network every $C$ steps.

\subsubsection{Implementation} \label{DQN-Implementation}

Deep Q-Learning was first developed utilising the Simple Push environment described in Section \ref{Method} allowing for quicker testing of the algorithm.

For each environment each agent type shared a neural net which consisted of an input layer which matched the dimensions of the observation of the environment state that an agent had, followed by three dense fully connected layers and then a final output layer which matched the action space of the agent.
In Simple World Comm the adversary leader did not share a net, the three adversaries shared a single net, and the good agents shared a single net.
By sharing the neural net between agent types it allowed the teams the benefit of learning from the actions that every agent on that team took and the rewards that earned.

The final hyperparameters that the algorithm was run with can be found in table \ref{tab:DQNHYPER} in Appendix B.


\subsection{Actor Critic} \label{A2C-Method}

Touched upon in Section \ref{Background}, PPO algorithms are policy gradient algorithms which aim to produce better results by limiting an agent's learning rate.
These algorithms curb changes to policy, preventing a new policy from deviating too far from the one that preceded it.
This is achieved through modification of the objective function.
For the PPO-clip algorithm this is given by:

\begin{equ}[!ht]
  \begin{equation}
    L^{CLIP}\theta = \hat{\mathbb{E}}[min(r(\theta)\hat{A_t},clip(r_t(\theta),1-\epsilon,1+\epsilon)\hat{A_t})]
    \label{equ:CLIP}
  \end{equation}
  \caption*{Clipped Surrogate Objective \citet{PPOAlgo}}
\end{equ}

Where the advantage function $A^t$ is given by a truncated version of the Generalized Advantage Function (GAE)

\begin{equ}[!ht]
  \begin{equation}
    \begin{aligned}
    A^t & = \delta_t + (\gamma\lambda)\delta_{t+1}+...+...+(\gamma\lambda)^{T-t+1}\delta_{T-1},\\
        & where\, \delta_t = r_t +\gamma V(S_{t+1})-V(s_t)\\
        &  \lambda = smoothing factor
    \end{aligned}
  \end{equation}
  \caption*{Truncated GAE \citet{PPOAlgo}}
\end{equ}


And $r_t(\theta)$ is the ratio of the new policy to the previous.

\begin{equ}[!ht]
  \begin{equation}
    \frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)}
  \end{equation}
\end{equ}

The first term in the Clipped Surrogate Objective function (equation \ref{equ:CLIP}) gives the normal policy gradient objective, the second gives us a clipped policy gradient objective.
The latter is equivalent to the former with the additional application of a clipping operation within the range of $1\pm\epsilon$ for hyperparameter $\epsilon$.
This clipping operation is visualised below.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=1]{clipping_graph.png}
  \caption{Plots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of the probability ratio r, for positive advantages (left) and negative advantages (right).
          The red circle on each plot shows the starting point for the optimisation, i.e., r = 1.
          Note that LCLIP sums many of these terms.” \citet{PPOAlgo}}
  \label{fig:clip_graph}
\end{figure}

A minimum is then computed over these two terms; in this way, the growth of $r_t(\theta)$ is constrained by an upper bound and the policy cannot change excessively after a single alteration.
This helps to stabilize training via constraining the policy changes at each step. This is useful as our gradient is only an approximation so trajectories which have high variance and large steps could be harmful to the policy.
Because of this PPO can perform multiple epochs of mini-batch updates, compared to a single policy gradient update per data sample in usual policy gradient methods.
This allows for greater learning per data sample and therefore a greater learning performance.

This algorithm has a number of strengths which make it particularly useful.
Maintaining conservative policy updates as detailed above provides better sample efficiency allowing the algorithm to converge more easily on an optimum.
Further, this algorithm was designed to be simpler to implement than the comparable Trust Region Policy Optimisation (TRPO) algorithm \citet{trustregionpolicy} which also provides better sample efficiency than previous policy gradient algorithms “and have better overall performance” \citet{PPOAlgo}.
More relevant to this project, \citet{emergenttoolusage} have explored the application of this algorithm to a 'hide-and-seek' problem, producing positive results in an environment similar to that which is explored here.
Given that this algorithm is known to perform well in a similar environment and the various strengths of the PPO-clip algorithm, it was chosen for further investigation in this project.

\subsubsection{Implementation} \label{A2C-Implementation}

As with the deep Q learning algorithm discussed in subsection \ref{DQN-Implementation} the A2C algorithm was initially applied to the Simple Push.
When applied to Simple Push the episode length was 250 steps.
The hyperparameters for this algorithm can be found in table \ref{tab:PPOHYPER} in Appendix B

The Simple World Comm environment was set up as described in Section \ref{Background}.
Episodes were set to a length of 500 steps.
Each type of the agent type in the environment uses the same actor and critic network.
Therefore, in the implementation of 1 lead adversary, 3 adversaries and 2 (good) agents there are 3 sets of actor and critic networks.


The actor and critic network had the same input neuron number, being the size of the agent's state space.
They also had the same hidden layer structure consisting of 2, 256 neuron layers both with Rectified Linear Unit activation functions.
The actor network having a dense output layer with the same number of neurons as the number of actions in the agent's action space and a SoftMax activation function giving an effective probability distribution of taking each action.
The critic network has a single output with no activation function as it evaluates a particular state. They were both compiled and optimised with an Adam optimizer \citet{adam-optimiser} with the above learning rate, $\alpha$.

A high-level program flow shows each agent to take an action based on their current policy.
After this the current state, output of each actor and critic network, action taken and reward from taking the action are recorded.
It was chosen that the learning part of the algorithm would be carried out every 20-time steps, using mini-batches of 5 which are run for 4 epochs.
This means that for every 20 steps taken the lead adversary has a single dataset to learn from, the adversaries have 3 and the agents have 2.
It was chosen to be implemented this way instead of individual networks per agent, for the obvious reason that each policy network will learn more in the same number of learning steps for the agent and adversaries.

\section{Results} \label{Results}

\subsection{Simple Push Environment}

Both the PPO and the deep Q learning algorithms showed good learning results in the simple push environment as can be seen from figures \ref{fig:DQNPUSH} and \ref{fig:PPOPUSH}.


\begin{figure}[!ht]
\hspace*{-1cm}
  \centering
  \begin{minipage}{.6\linewidth} 
    \centering
    \includegraphics[width=\linewidth]{push_DQN.png}
    \captionof{figure}{Learning curve from simple push \\
                      environment using the deep Q learning algorithm. \\
                      Cyan and dark red lines denote random policy baselines.}
    \label{fig:DQNPUSH}
  \end{minipage}%
  \begin{minipage}{.6\linewidth}
    \centering
    \includegraphics[width=\linewidth]{push_PPO.png}
    \captionof{figure}{Learning curve from simple push environment using the PPO algorithm. \\
            Cyan and dark red lines denote random policy baselines.}
    \label{fig:PPOPUSH}
  \end{minipage}
\end{figure}


Both show convergence to an optimal policy for both the agent and adversary, with the deep Q learning reaching a seemingly optimal policy within 150 episodes, while the PPO seems to take around 600 episodes for both agent and adversary to converge.

\subsection{Simple World Comm Environment}

The same algorithms were then applied to the simple world comm environment.
In this the PPO algorithm performed better with the learning curves for both the agent and the adversary showing good learning during the initial ~100 episodes and convergence to a seemingly optimal policy by ~250 episodes.




\begin{figure}[!ht]
  \hspace*{-1cm}
  \centering
  \begin{minipage}{.6\linewidth} 
    \centering
    \includegraphics[width=\linewidth]{world_comm_DQN.png}
    \captionof{figure}{Learning curve from simple world \\
                      comm environment using the deep Q learning \\ 
                      algorithm. Cyan and dark red lines denote \\ 
                      random policy baselines.}
    \label{fig:DQNCOMM}
  \end{minipage}%
  \begin{minipage}{.6\linewidth}
    \centering
    \includegraphics[width=\linewidth]{world_comm_PPO.png}
    \captionof{figure}{Learning curve from simple world \\
                      comm environment using the PPO algorithm. \\
                      Cyan and dark red lines denote random \\
                      policy baselines.}
    \label{fig:PPOCOMM}
  \end{minipage}
\end{figure}


% \begin{figure}[!ht]
%     \centering
%     \includegraphics[scale=.7]{simple_world_comm_DQN.png}
%     \caption{Learning curve from simple world comm environment using the DQN algorithm. 
%                       Cyan and darker red lines denote random policy baselines.}
%     \label{fig:DQNCOMM}
% \end{figure}

% \begin{figure}[!ht]
%   \centering
%   \includegraphics[scale=.7]{simple_world_comm_PPO.png}
%   \caption{Learning curve from simple world comm environment using the PPO algorithm.
%                     Cyan and darker red lines denote random policy baselines.}
%     \label{fig:PPOCOMM}
% \end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=.7]{strat_development.PNG}
  \caption{Learning curve showing possible strategy development of agent and adversary}
  \label{fig:STRATLEARN}
\end{figure}

The other main feature of the learning curves for the PPO algorithm is when the comparison of rewards for the agent and adversary are compared once the initial learning phase has been completed.
As we can see in figure \ref{fig:STRATLEARN}, there is a clear oscillation of the reward curves for the agent and adversary.
These responses have been observed in similar MARL environments \citep{emergenttoolusage}, and represent the development of a new strategy and then this being countered and overcome by the opposing agents/adversary.
This strategy development seems to have at least five phases during the training and have been highlighted in the chart reflect the agent or adversary that appears to have an advantage during these episodes.


\section{Discussion} \label{Discussion}

We have produced two algorithms that have demonstrated reinforcement learning in a multiagent, complex and competitive environment.
The results also suggests the development of new strategies to maximise rewards, that is then countered/overcome by the opposing adversarial agents.
The potentially mimics the evolutionary 'arms race' seen between competing organisms directed by natural selection \citet{armsrace}.

While the learning of a strategy can be clearly observed in section \ref{Results} one thing that should be noted is that both teams of agents were learning in parallel.
This likely has at least a small stifling effect on new strategies being developed either by the agents or the adversaries because as soon as a new strategy begins to emerge the other team immediately begins to counter it.
Potentially interesting swings in the relative team score that could have been observed if only a single team was learning at any given time is lost if the freedom for strategies to be fully developed is not there.

\section{Future Work} \label{Future Work}

Future work on this project could involve analysis of the strategies that have developed at specific episode intervals as has been documented in \citet{emergenttoolusage}.
Possible examples of strategies might be the running and chasing of the agents and adversaries, the agents hiding in the 'shrubberies' to avoid detection, development of the adversary leader communicating locations to the other adversaries.
At the moment only the rewards for each episode are recorded, but to allow analysis of the strategies monitoring of other in-episode parameters could be applied.
Examples might be recording the amount of movement of the agents/ adversaries, distance of the agents to the 'shrubberies', communication and response of the adversaries.

A potential extension of the analysis of how the teams learn should be carried out, this would consist of not training the two teams simultaneously but instead allowing one team to train for $x$ number of episodes before the other team trains for a similar number of episodes.
Currently, from the agents' team view the way that the environment reacts to their actions constantly changes, so it is difficult for a fully optimal policy to be reached unlike when an agent is learning against an environment which behaves either identically or similarly in every episode.


\section{Personal Experience} \label{Personal Experience}

Will, not being from any of the data science tracks, found the project to be a very steep learning curve in how to implement neural nets using Tensorflow and Keras having not used them before.
It took some time for him to get his head round the syntax and what was required in terms of activation functions but when he did and had implemented the PPO algorithm with GAE (having seen it in the hide-and-seek paper \citep{emergenttoolusage} which was similar to our problem), his work produced good results and some surprisingly pleasant learning curves. This learning has resulted in him having a good grasp of his chosen policy gradient method!

An ongoing frustration within the team was the long training cycles that the algorithms (deep Q learning in particular) had to achieve any kind of reasonable results, often in excess of 10 hours using available compute hardware which made iterating on different ideas difficult within the timeframe of the project. There are a lot of ideas which it would have been beneficial to explore but were not possible with the available time and computing power.


% \subsection{Mark Hazell}

% In many ways a primary hallmark of my experience in this project has been one of frustration and waiting.
% I am used to being able to rapidly iterate on ideas and code however while implementing the DQN algorithm I was often faced with long training times in the double-digit hours to achieve a reasonable number of episodes.
% This often led to the completion of a training session before noticing some manner in which the algorithm could be improved for performance or not quite correct and having to repeat this training multiple times.
% However, in spite of this I have learnt a lot about how to implement the algorithms discussed in this report as well as how to break down a more complex reinforcement learning problem into constitute parts (going from Simple World Comm to Simple Push) to prove that what is being implemented is indeed functional.


% \subsection{Adam Rasool}

% \subsection{Will Prior}

% We went for the latter found to be rather ambitious task of a multiagent environment.
% I was tasked with looking into the effectiveness of policy gradient algorithms on environment.
% After implementing both the REINFORCE and a basic actor-critic algorithm with no noticeable learning/ consistency in the results, we went back to the drawing board.
% In this we found the hide-and-seek paper that has been mentioned previously in the report which was a very similar experiment to that of our problem.
% In this it noted about how the use of PPO with GAE was an algorithm that worked well in the environment - it seemed logical to implement this in our environment.
% Which I did, giving the pleasant surprise of some nice learning curves as seen in the results section.
% Giving good performance for both simple push and simple world comm.
% This project was a very steep learning curve in how to implement neural nets using TensorFlow and Keras, which took some time to get my head round the syntax and what was required in terms of activation functions number of neurons etc.
% However, I now feel I have a good grasp of this along with a rather advanced policy gradient method!


\bibliographystyle{bath}
\bibliography{references}
\small

\normalsize
\newpage

\section*{Appendices}
\subsection*{Appendix A: Environment Cycle}

Simple World Comm consists of six agents, two on the "good" team and four on the "adversary" team.
Good agents are rewarded for proximity to a 'food' object(s), adversaries that are rewarded for collisions with good agents, and also there are two additional type of objects, a barrier that blocks the way and two 'shrubberies' that hide the position of the agents from observation.
One of the adversary agents is leader adversary that can always see the location of the good agents and communicate that location to the other adversary agents.

Good agents will attempt to maximise their reward by remaining proximal to the 'food' while also attempting to minimise the number of collisions with adversaries.
Adversaries on the other hand will be focused on attempting to collide and remain as close as possible with the good agents.

While the environment exists on an infinite plain, the good agents are incentivised to not move too far away from the origin point by an increasingly large negative reward every step once a threshold is crossed proportional to the distance from the origin.
Adversarial agents are not penalised in this manner but since they are already rewarded for attempting to collide with the good agents, if the good agents remain within the bounded area so should all the adversaries.

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.25]{simple_comm_environment.png}
  \caption{Simple Comm World Environment}
  \label{fig:simple_comm_world_2}
\end{figure}

The transition dynamics for the environment is a cycle as depicted in figure \ref{fig:stepcycle} found in Appendix A where each agent gets to take an action, once all agents have acted that step can be considered completed beginning a new step and cycle.
The default episode length for the environments is 25 steps however this was extended in this project to a minimum length of 50 depending on the algorithm used. 
This difference was primarily dictated by training time.

\clearpage

\begin{figure}[!ht]
  \centering
  \includegraphics[scale=0.5]{Cycle_diagram.png}
  \caption{Environment transition dynamics.}
  \label{fig:stepcycle}
\end{figure}

The adversary and good agents both share the same action of space of either taking no action or, moving in the cardinal directions.

\begin{table}[!ht]
  \centering
  \begin{tabular}{|c|}
  \hline
  Good Agent  \\ \hline
  no\_action  \\ \hline
  move\_left  \\ \hline
  move\_right \\ \hline
  move\_up    \\ \hline
  move\_down  \\ \hline
\end{tabular}
\caption{table}{Good agent action space}
\label{tab:good-agent-action}
\end{table}

\begin{table}[!ht]
  \centering
  \begin{tabular}{|c|}
    \hline
    Adversary Agent  \\ \hline
    no\_action  \\ \hline
    move\_left  \\ \hline
    move\_right \\ \hline
    move\_up    \\ \hline
    move\_down  \\ \hline
    \end{tabular}
    \caption{Adversary agent action space}
    \label{tab:adversary-agent-action}
  \end{table}

  The adversary leader's action space is the product of the movement actions that the other agents have and also the ability to either not communicate or communicate with one of the other adversary agents.

\begin{table}[!ht]
  \centering
  \addtolength{\leftskip} {-2cm}
  \addtolength{\rightskip}{-2cm}
  \begin{tabular}{|l|l|l|l|l|}
    \hline
    Adversary Leader  & say\_0                & say\_1                & say\_2                & say\_3                \\ \hline
    no\_action  & (no\_action, say\_0)  & (no\_action, say\_1)  & (no\_action, say\_2)  & (no\_action, say\_3)  \\ \hline
    move\_left  & (move\_left, say\_0)  & (move\_left, say\_1)  & (move\_left, say\_2)  & (move\_left, say\_3)  \\ \hline
    move\_right & (move\_right, say\_0) & (move\_right, say\_1) & (move\_right, say\_2) & (move\_right, say\_3) \\ \hline
    move\_up    & (move\_up, say\_0)    & (move\_up, say\_1)    & (move\_up, say\_2)    & (move\_up, say\_3)    \\ \hline
    move\_down  & (move\_down, say\_0)  & (move\_down, say\_1)  & (move\_down, say\_2)  & (move\_down, say\_3)  \\ \hline
    \end{tabular}
    \caption{Adversary leader action space}
    \label{tab:adversary-leader-action}
  \end{table}

  Good agents are rewarded for both colliding with the food objects and also maintaining a position within proximity of the food.
  The reward for being proximal to the food decreasing with distance.
  Good agents are punished with a negative reward colliding with adversaries.
  Adversaries are rewarded for colliding with good agents while earning a punishment each step in the form of a negative reward proportional to how close good agents are to the food.

  \begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|}
    \hline
    Good Agent      & \begin{tabular}[c]{@{}l@{}}-5 per collision with \\ an adversary agent\end{tabular} & \begin{tabular}[c]{@{}l@{}}+2 per collision with\\ food object\end{tabular}   & \begin{tabular}[c]{@{}l@{}}-0.05 x min distance to\\ a good object\end{tabular} \\ \hline
    Adversary Agent & \begin{tabular}[c]{@{}l@{}}+5 per collision with\\ a good agent.\end{tabular}       & \begin{tabular}[c]{@{}l@{}}-0.1 x min distance to\\ a good agent\end{tabular} &                                                                                 \\ \hline
    \end{tabular}
    \caption{Agent Rewards}
    \label{tab:agent-rewards}
    \end{table}

\clearpage

\subsection*{Appendix B: Algorithm Hyperparameters}

\begin{table}[!h]
  \centering
  \begin{tabular}{|l|l|}
  \hline
  \textbf{Hyperparameter}                      & \textbf{Value}          \\ \hline
  Initial Exploration Rate $\epsilon$          & 1                       \\ \hline
  Exploration Decay Rate                       & 0.05                    \\ \hline
  Minimum Exploration Rate $\epsilon$          & 0.1                     \\ \hline
  Decay Rate $\gamma$                          & 0.95                    \\ \hline
  Learning Rate $\alpha$                       & 0.00025                 \\ \hline
  Batch Size                                   & 8                       \\ \hline
  Target Network Update                        & Every 10 steps          \\ \hline
  \end{tabular}
  \caption{Deep Q-Learning hyperparameters}
  \label{tab:DQNHYPER}
  \end{table}

  \begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|}
      \hline
    \textbf{Hyperparameter}             & \textbf{Value}  \\ \hline
    Discount Factor, $\gamma$           & 0.99            \\ \hline
    Learning Rate, $\alpha$             & 0.0003          \\ \hline
    Smoothing Factor, $\lambda$         & 0.95            \\ \hline
    Policy Clip, $\epsilon$             & 0.2             \\ \hline
    Batch Size                          & 20              \\ \hline
    Mini-batch Size                     & 5               \\ \hline
    Number of epochs                    & 4               \\ \hline
    \end{tabular}
    \caption{Actor Critic Hyper Parameter Values}
    \label{tab:PPOHYPER}
  \end{table}

\end{document}
