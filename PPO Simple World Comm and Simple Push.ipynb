{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation of the PPO Clip Algorithm on Pettingzoo's Simple World Comm Multi-Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install tensorflow_probability\n",
    "!pip install pettingzoo[mpe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from pettingzoo.mpe import simple_world_comm_v2, simple_push_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Classes\n",
    "These classes intialise and call the seperate actor and critic networks of the algorithm. They both use the same structured network of an input layer of the current agents state two dense hidden layers of 256 nuerons each as default both using the Rectified Linear Unit (relu) activation functions. For the actor the final output layer is a dense layer, with the same number of neurons as there is actions in the current agents action space. It uses a softmax activation function causing all the output values to sum to one, giving an effective distribution with the larger favoured action though to obtain the largest reward. The critic has a single output as it outputs the \"value\" of the particular state being evaluted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Net(keras.Model):\n",
    "    def __init__(self, n_actions, layer_1_neur=256, layer_2_neur=256):\n",
    "        super(Actor_Net, self).__init__()\n",
    "\n",
    "        self.layer_1 = Dense(layer_1_neur, activation='relu')\n",
    "        self.layer_2 = Dense(layer_2_neur, activation='relu')\n",
    "        self.layer_3 = Dense(n_actions, activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.layer_1(state)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic_Net(keras.Model):\n",
    "    def __init__(self, layer_1_neur=256, layer_2_neur=256):\n",
    "        super(Critic_Net, self).__init__()\n",
    "        self.layer_1 = Dense(layer_1_neur, activation='relu')\n",
    "        self.layer_2 = Dense(layer_2_neur, activation='relu')\n",
    "        self.layer_3 = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.layer_1(state)\n",
    "        x = self.layer_2(x)\n",
    "        q = self.layer_3(x)\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Networks\n",
    "To deal with the multi agent environemnt in which each type of agent will need its own set of actor and critic networks the following functions were created to initialise the 3 respective actor and critic networks required for the learning for the leadadversary, the adversaries and the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_nets(env, agent, alpha):\n",
    "    actor_net = Actor_Net(env.action_space(agent).n)\n",
    "    actor_net.compile(optimizer=Adam(learning_rate=alpha))\n",
    "    critic_net = Critic_Net()\n",
    "    critic_net.compile(optimizer=Adam(learning_rate=alpha))\n",
    "    return [agent, actor_net, critic_net]\n",
    "\n",
    "def create_agent_nets(env, alpha):\n",
    "    agent_nets = []\n",
    "    for agent in env.agents:\n",
    "        if \"leadadversary_\" in agent:\n",
    "            try:\n",
    "                lead_adversary_Nets\n",
    "            except:\n",
    "                lead_adversary_Nets = compile_nets(env, agent, alpha)\n",
    "                agent_nets.append(lead_adversary_Nets)\n",
    "        elif \"adversary_\" in agent:\n",
    "            try:\n",
    "                adversary_Nets\n",
    "            except:\n",
    "                adversary_Nets = compile_nets(env, agent, alpha)\n",
    "                agent_nets.append(adversary_Nets)\n",
    "        elif \"agent_\" in agent:\n",
    "            try:\n",
    "                agent_Network\n",
    "            except:\n",
    "                agent_Network = compile_nets(env, agent, alpha)\n",
    "                agent_nets.append(agent_Network)\n",
    "    return agent_nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Storing Class\n",
    "Stores each batch of episode steps before learning takes place.\n",
    "Generates \"minibatches\" of shuffled data to run in each epoch during learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class n_step_Memory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def create_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "            np.array(self.actions),\\\n",
    "            np.array(self.probs),\\\n",
    "            np.array(self.vals),\\\n",
    "            np.array(self.rewards),\\\n",
    "            np.array(self.dones),\\\n",
    "            batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO agent Class\n",
    "Contains functions which:\n",
    "- Assigns the agent the appropriate set of neural nets to make sure each type of agent only trains one set\n",
    "- chooses the action the next action of the agent using its policy and its current state\n",
    "- Updates the agents actor and critic networks one using the clip objective function and the other using a mean squared error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, agent, agent_nets, n_actions, input_dims, gamma=0.99, alpha=0.0003,\n",
    "                   lamda=0.95, epsilon=0.2, batch_size=64,\n",
    "                 n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lamda = lamda\n",
    "\n",
    "        self.agent = agent\n",
    "        self.actor, self.critic = self.assign_agent_nets(agent_nets)\n",
    "        self.memory = n_step_Memory(batch_size) \n",
    "        self.score_history = []\n",
    "        self.score = 0\n",
    "        self.action = None\n",
    "        self.probs = None\n",
    "        self.val = None\n",
    "\n",
    "        self.learn_iters =  0\n",
    "        self.avg_score = 0\n",
    "        self.n_steps = 0\n",
    "        self.observation = None\n",
    "        \n",
    "    def assign_agent_nets(self, agent_nets):\n",
    "        for agent_net in agent_nets:\n",
    "            if (\"leadadversary_\" in agent_net[0]) and (\"leadadversary_\" in self.agent):\n",
    "                return agent_net[1], agent_net[2]\n",
    "            elif \"adversary_\" in agent_net[0] and (\"adversary_\" in self.agent) and (\"leadadversary_\" not in agent_net[0]):\n",
    "                return agent_net[1], agent_net[2]\n",
    "            elif \"agent_\" in agent_net[0] and (\"agent_\" in self.agent):\n",
    "                return agent_net[1], agent_net[2]\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        \n",
    "        action_probs = self.actor(state)\n",
    "        cat_probs = tfp.distributions.Categorical(action_probs)\n",
    "        action = cat_probs.sample()\n",
    "        log_prob = cat_probs.log_prob(action)\n",
    "        crit_val = self.critic(state)\n",
    "\n",
    "        action = action.numpy()[0]\n",
    "        crit_val = crit_val.numpy()[0]\n",
    "        log_prob = log_prob.numpy()[0]\n",
    "\n",
    "        return action, log_prob, crit_val \n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "                reward_arr, dones_arr, batches = \\\n",
    "                self.memory.create_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            A_hat = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "            #calculate GAE advanatage estimates \n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                A_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    A_t += discount*(reward_arr[k] + self.gamma*values[k+1] * (\n",
    "                        1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.lamda\n",
    "                A_hat[t] = A_t\n",
    "            \n",
    "            # start each epoch of learning \n",
    "            for minibatch in batches:\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    states = tf.convert_to_tensor(state_arr[minibatch])\n",
    "                    old_probs = tf.convert_to_tensor(old_prob_arr[minibatch])\n",
    "                    actions = tf.convert_to_tensor(action_arr[minibatch])\n",
    "\n",
    "                    probs = self.actor(states)\n",
    "                    dist = tfp.distributions.Categorical(probs)\n",
    "                    new_probs = dist.log_prob(actions)\n",
    "\n",
    "                    critic_value = self.critic(states)\n",
    "\n",
    "                    critic_value = tf.squeeze(critic_value, 1)\n",
    "                    \n",
    "                    # calculate the two ratios - inital is exponential as currently logged\n",
    "                    r_theta = tf.math.exp(new_probs) / tf.math.exp(old_probs)\n",
    "                    \n",
    "                    # this is the implementation of the Lclip objective function \n",
    "                    weighted_r_theta = A_hat[minibatch] * r_theta\n",
    "                    weighted_clipped_r_theta = A_hat[minibatch] * tf.clip_by_value(r_theta,\n",
    "                                                     1-self.epsilon,\n",
    "                                                     1+self.epsilon)\n",
    "                    \n",
    "                    # negative as it is doing gradient ascent rather then descent\n",
    "                    actor_loss = -tf.math.minimum(weighted_r_theta,\n",
    "                                                  weighted_clipped_r_theta)\n",
    "                    actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "                    \n",
    "                    # used mean square error for the critic loss \n",
    "                    returns = A_hat[minibatch] + values[minibatch]\n",
    "            \n",
    "                    critic_loss = keras.losses.MSE(critic_value, returns)\n",
    "\n",
    "                actor_params = self.actor.trainable_variables\n",
    "                actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "                critic_params = self.critic.trainable_variables\n",
    "                critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "                self.actor.optimizer.apply_gradients(\n",
    "                        zip(actor_grads, actor_params))\n",
    "                self.critic.optimizer.apply_gradients(\n",
    "                        zip(critic_grads, critic_params))\n",
    "\n",
    "        self.memory.clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### House Keeping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(agent_net, episode):\n",
    "    print('... saving models ...')\n",
    "    agent_net[1].save(save_model_path + \"ep_\"+ str(episode)+\"/\" + agent_net[0] + \"_\" + 'PPO_actor_simp_world_ep_'+ str(episode))\n",
    "    agent_net[2].save(save_model_path + \"ep_\"+ str(episode)+\"/\" + agent_net[0] + \"_\" + 'PPO_critic_simp_world_ep_'+ str(episode))\n",
    "\n",
    "def load_models(agent_name, episode):\n",
    "    print('... loading models ...')\n",
    "    actor = keras.models.load_model(save_model_path + \"ep_\"+ str(episode)+\"/\" + agent_name + \"_\" + 'PPO_actor_simp_world_ep_'+ str(episode))\n",
    "    critic = keras.models.load_model(save_model_path + \"ep_\"+ str(episode)+\"/\" + agent_name + \"_\" + 'PPO_critic_simp_world_ep_'+ str(episode))\n",
    "    return [agent_name, actor, critic]\n",
    "\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)\n",
    "\n",
    "def run_plot(agents, figure_file):\n",
    "    for agent in agents:\n",
    "        x = [i+1 for i in range(len(agent.score_history))]\n",
    "        plot_learning_curve(x, agent.score_history, figure_file)\n",
    "        \n",
    "def create_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(\"Created Directory : \", dir)\n",
    "    else:\n",
    "        print(\"Directory already existed : \", dir)\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm\n",
    "Below runs the PPO-Clip algorithm for \n",
    "- the Simple World Comm environment https://www.pettingzoo.ml/mpe/simple_world_comm\n",
    "- the simple push environment https://www.pettingzoo.ml/mpe/simple_push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    episodes = 2001\n",
    "    \n",
    "    #################### below runs simple world comm ###########################\n",
    "    \n",
    "    MAX_STEPS = 500\n",
    "    save_model_path = \"models/simple_world_comm/\"\n",
    "    env = simple_world_comm_v2.env(num_good=2, num_adversaries=4, num_obstacles=1,\n",
    "                num_food=2, max_cycles=MAX_STEPS, num_forests=2, continuous_actions=False)\n",
    "   \n",
    "    ##################uncomment below and comment above to run simple push ################### \n",
    "    \n",
    "    #MAX_STEPS = 250   \n",
    "    #save_model_path = \"models/simple_push/\"  \n",
    "    #env = simple_push_v2.env(max_cycles=MAX_STEPS, continuous_actions=False)\n",
    "    \n",
    "    #########################################################################################\n",
    "    \n",
    "    \n",
    "    create_dir(save_model_path)\n",
    "    env.reset()\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    agent_nets = create_agent_nets(env, alpha) \n",
    "    \n",
    "    ##code for loading a model if run crashes \n",
    "   # model_names = [\"leadadversary_0\", \"adversary_0\", \"agent_0\"]\n",
    "    \n",
    "  #  for model in model_names:\n",
    "   #     agent_nets.append(load_models(savmodel, 700))\n",
    "     \n",
    "    agents =[]\n",
    "    for agent in env.agents:\n",
    "        agents.append(PPOAgent(agent, agent_nets, env.action_space(agent).n, env.observation_space(agent).shape[0], batch_size=batch_size,\n",
    "                  alpha=alpha, n_epochs=n_epochs))\n",
    "    \n",
    "    \n",
    "\n",
    "    for episode in range(0, episodes):\n",
    "        env.reset()\n",
    "        for agent in agents:\n",
    "            agent.score = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            for agent in agents:\n",
    "                agent.observation = env.observe(agent.agent)\n",
    "                agent.action, agent.prob, agent.val = agent.choose_action(agent.observation)\n",
    "                env.step(agent.action)\n",
    "            \n",
    "            for agent in agents:\n",
    "                done = env.dones[agent.agent]\n",
    "                reward = env.rewards[agent.agent]\n",
    "                agent.n_steps += 1\n",
    "                agent.score += reward\n",
    "                agent.memory.store_memory(agent.observation, agent.action, agent.prob, agent.val, reward, done)\n",
    "                if agent.n_steps % N == 0:\n",
    "                    agent.learn()\n",
    "                    agent.learn_iters += 1\n",
    "                    \n",
    "        for agent in agents:\n",
    "            \n",
    "            agent.score_history.append(agent.score)\n",
    "            agent.avg_score = np.mean(agent.score_history[-100:])\n",
    "                \n",
    "            log = agent.agent + \" episode \" + str(episode) + \" score \" + str(agent.score) + \" avg score \" + str(agent.avg_score) +\" time_steps \" + str(agent.n_steps) + \" learning_steps \" + str(agent.learn_iters)\n",
    "            print(log)\n",
    "            text_file = open(save_model_path+\"learning_log.txt\", \"a\")\n",
    "            log = log + \"\\n\"\n",
    "            n = text_file.write(log)\n",
    "            text_file.close()\n",
    "            \n",
    "        if episode % 50 == 0:\n",
    "            for agent in agent_nets:\n",
    "                save_models(agent, episode)\n",
    "            figure_file = save_model_path + 'ep_'+ str(episode)+'/Avg_Learing_plot_ep_' + str(episode)+ '.png'\n",
    "            run_plot(agents,figure_file)\n",
    "            \n",
    "    figure_file = save_model_path +'Avg_Learing_plot_FINAL.png'            \n",
    "    run_plot(agents, figure_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits\n",
    "philtabor - https://github.com/philtabor/Youtube-Code-Repository/tree/master/ReinforcementLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
