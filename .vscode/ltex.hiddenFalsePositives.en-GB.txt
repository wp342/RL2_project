{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QSteven Etches spe24@bath.ac.uk Mark Hazell mph55@bath.ac.uk Adam Rasool aar73@bath.ac.uk Will Prior wp342@bath.ac.uk\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QThe reinforcement learning problem that we have chosen to address is from the multiagent reinforcement learning (MARL) environments provided in PettingZoo.ml by \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"LC_AFTER_PERIOD","sentence":"^\\QThe reinforcement learning problem that we have chosen to address is from the multiagent reinforcement learning (MARL) environments provided in PettingZoo.ml by \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QOwing to their relative simplicity in addition to their effectiveness \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q, these algorithms are popular, with research available which extends their application to multiagent systems such as that discussed in \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q which outlines the multiagent PPO algorithm MAPPO.\\E$"}
{"rule":"MISSING_GENITIVE","sentence":"^\\QHuber loss (green) and squared error loss (blue) as a function of y - f(x) \\E(?:Dummy|Ina|Jimmy-)[0-9]+$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QPlots showing one term (i.e., a single timestep) of the surrogate function LCLIP as a function of the probability ratio r, for positive advantages (left) and negative advantages (right).\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QNote that LCLIP sums many of these terms.”\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QFurther, this algorithm was designed to be simpler to implement than the comparable Trust Region Policy Optimisation (TRPO) algorithm \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q which also provides better sample efficiency than previous policy gradient algorithms “and have better overall performance” \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q.\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QThey also had the same hidden layer structure consisting of 2, 256 neuron layers both with Rectified Linear Unit (relu) activation functions.\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QThe actor network having a dense output layer with the same number of neurons as the number of actions in the agent's action space and a SoftMax activation function giving an effective probability distribution of taking each action.\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QAdam Rasool.\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QGood Agent noaction moveleft moveright moveup movedown table Good agent action space\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QAdversary Agent noaction moveleft moveright moveup movedown Adversary agent action space\\E$"}
{"rule":"MORFOLOGIK_RULE_EN_GB","sentence":"^\\QAdversary Leader say0 say1 say2 say3 noaction (noaction, say0) (noaction, say1) (noaction, say2) (noaction, say3) moveleft (moveleft, say0) (moveleft, say1) (moveleft, say2) (moveleft, say3) moveright (moveright, say0) (moveright, say1) (moveright, say2) (moveright, say3) moveup (moveup, say0) (moveup, say1) (moveup, say2) (moveup, say3) movedown (movedown, say0) (movedown, say1) (movedown, say2) (movedown, say3) Adversary leader action space\\E$"}
{"rule":"ENGLISH_WORD_REPEAT_RULE","sentence":"^\\Q-0.1 x min distance to a good agent Agent Rewards\\E$"}
{"rule":"CD_NN","sentence":"^\\QHyperparameter Value Discount Factor, \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q 0.99 Learning Rate, \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q 0.0003 Smoothing Factor, \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q 0.95 Policy Clip, \\E(?:Dummy|Ina|Jimmy-)[0-9]+\\Q 0.2 Batch Size 20 Mini-batch Size 5 Number of epochs 4 Actor Critic Hyper Parameter Values\\E$"}
{"rule":"UPPERCASE_SENTENCE_START","sentence":"^\\Qequ[][]\\E$"}
