{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6720f11",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7aa313",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To be able to run this notebook properly please make sure to install the pettingzoo package and dependencies. This can be done by running the following command\n",
    "\n",
    "`pip install pettingzoo[mpe]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc3cd2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eecab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_world_comm_v2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4441908",
   "metadata": {},
   "source": [
    "### Environment Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CYCLES = 250\n",
    "NUM_OF_EPISODES = 10\n",
    "\n",
    "\n",
    "env = simple_world_comm_v2.env(num_good=2, num_adversaries=4, num_obstacles=1,\n",
    "                num_food=2, max_cycles=MAX_CYCLES, num_forests=2, continuous_actions=False, seed=42)\n",
    "env.reset()\n",
    "print(f\"Agents: {env.agents}\")\n",
    "print()\n",
    "agent_mapping = {k: v for v, k in enumerate(env.agents)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082db82",
   "metadata": {},
   "source": [
    "### Policy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(actions):\n",
    "    return random.randint(0, actions-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765696b",
   "metadata": {},
   "source": [
    "### Inspection Functions\n",
    "\n",
    "A collection of inspection functions to help minimise clutter in the training loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350603b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_agent_only(agent, desired_agent, function, function_args):\n",
    "#     To use this function, pass the agent and desired agent as the first two arguments,\n",
    "#     then pass the function reference and the arguments for the function as a tuple for the final argument.\n",
    "    if agent == desired_agent:\n",
    "        function(*function_args)\n",
    "\n",
    "\n",
    "def print_agent_rewards(agent, reward):\n",
    "    print(f\"{agent}:{reward}\")\n",
    "    \n",
    "def print_agent_state(agent, observation):\n",
    "    print(f\"{agent}: {observation}\")\n",
    "    \n",
    "def print_iter_info(agent,observation,reward,done,info):\n",
    "    print(f\"Current Agent: {agent}\")\n",
    "    print(f\"Obs: {observation}\")\n",
    "    print(f\"Rew: {reward}\")      \n",
    "    print(f\"Done: {done}\")\n",
    "    print(f\"Info: {info}\")\n",
    "\n",
    "def get_current_step(env):\n",
    "    return env.env.env.steps\n",
    "\n",
    "def np_array_no_e(array):\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(array)\n",
    "    np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a679a49",
   "metadata": {},
   "source": [
    "### Running the environment\n",
    "\n",
    "The `env.render(mode='human')` call will pop open a new window that shows the environment at each time step.\n",
    "\n",
    "On my machine at least this window can only be closed while the cell is running but then freezes and is unable to be closed afterwards. In these cases restarting the kernel closed the window and any others which may have been opened due to running the cell multiple times.\n",
    "\n",
    "Eventually running the cell enough times without restarting the kernal will cause the render call to throw an exception and not run. In this case just restart the kernal and it will begin working again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e78e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent_stub:\n",
    "    def __init__(self):\n",
    "        self.learning_steps = 123456789123456789\n",
    "    \n",
    "    def policy(self, state):\n",
    "        return random_policy(env.action_space(agent).n)\n",
    "    \n",
    "    def save_action_state(self, action, state):\n",
    "        pass\n",
    "    \n",
    "    def save_memory(self, state, reward, done):\n",
    "        pass\n",
    "    \n",
    "    def replay(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7447fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class good_agent_stub:\n",
    "    def __init__(self):\n",
    "        self.learning_steps = 123456789123456789\n",
    "    \n",
    "    def policy(self, state):\n",
    "        return 0\n",
    "    \n",
    "    def save_action_state(self, action, state):\n",
    "        pass\n",
    "    \n",
    "    def save_memory(self, state, reward, done):\n",
    "        pass\n",
    "    \n",
    "    def replay(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51582df2",
   "metadata": {
    "code_folding": [
     34
    ]
   },
   "outputs": [],
   "source": [
    "class good_agent:\n",
    "    \n",
    "    def __init__(self, agent_name, epsilon=1, epsilon_min = 0.1, \n",
    "                 epsilon_decay = 0.95, batch_size=16, learning_decay_rate = 0.95,\n",
    "                learning_steps = 25):\n",
    "        self.state_size = 28\n",
    "        self.action_space = 5\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "        self.loss_function = keras.losses.Huber()\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.agent_name = agent_name\n",
    "        self.agent_file = agent_name+\".h5\"\n",
    "        self.agent_target_file = agent_name+\"_target.h5\"\n",
    "        self.history = []\n",
    "        self.action_taken = None\n",
    "        self.previous_state = None\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = learning_decay_rate\n",
    "        self.learning_steps = learning_steps\n",
    "\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(32, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(64, activation=\"relu\"))\n",
    "        model.add(layers.Dense(64, activation=\"relu\"))\n",
    "        model.add(layers.Dense(self.action_space, activation=\"relu\"))\n",
    "        model.compile(loss=keras.losses.Huber(), \n",
    "                      optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0))\n",
    " \n",
    "        return model\n",
    "    \n",
    "    def exploration_decay(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = min(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "            \n",
    "    def save_model(self):\n",
    "        self.model.save(self.agent_file)\n",
    "        self.target_model.save(self.agent_target_file)\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.model.load_weights(self.agent_file)\n",
    "        self.target_model.load_weights(self.agent_target_file)\n",
    "        \n",
    "    def policy(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        action_vals = self.predict(state)\n",
    "        return np.argmax(action_vals[0])\n",
    "    \n",
    "    def save_action_state(self, action, state):\n",
    "        self.action_taken = action\n",
    "        self.previous_state = state\n",
    "        \n",
    "    def save_memory(self, state, reward, done):\n",
    "        if self.previous_state is None and self.action_taken is None:\n",
    "            self.previous_state = state\n",
    "            self.action_taken = 0\n",
    "        self.history.append((self.previous_state, self.action_taken, reward, state, done))\n",
    "    \n",
    "    def get_memory(self):\n",
    "        return self.history\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.history) < self.batch_size:\n",
    "            return\n",
    "        sample_batch = random.sample(self.history, self.batch_size)\n",
    "        for state, action, reward, next_state, done in sample_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.predict(next_state)[0])\n",
    "            target_f = self.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            state = np.reshape(state, (1,28))\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        self.exploration_decay()\n",
    "    \n",
    "    def get_learning_steps(self):\n",
    "        return self.learning_steps\n",
    "    \n",
    "    def predict(self, state):\n",
    "#         print(f\"Before Reshape: {state}\")\n",
    "        state = np.reshape(state, (1,28))\n",
    "#         print(f\"After Reshape: {state}\")\n",
    "        return self.model.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba70ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# print(NUM_OF_EPISODES)\n",
    "# print(len(env.agents))\n",
    "reward_array = np.zeros((NUM_OF_EPISODES,len(env.agents)))\n",
    "\n",
    "cumulative_reward = np.zeros(len(env.agents))\n",
    "i = 0\n",
    "agent_models = {}\n",
    "for key in agent_mapping:\n",
    "    if key == \"leadadversary_0\":\n",
    "        agent_models[key] = agent_stub()\n",
    "    if \"adversary\" in key:\n",
    "        agent_models[key] = agent_stub()\n",
    "    if \"agent\" in key:\n",
    "        if \"agent_0\" == key:\n",
    "#             print(\"Making object\")\n",
    "            agent_models[key] = good_agent(key)\n",
    "        else:\n",
    "            agent_models[key] = good_agent_stub()\n",
    "    \n",
    "            \n",
    "            \n",
    "for episode in range(NUM_OF_EPISODES):\n",
    "    print(f\"Episode {episode+1} out of {NUM_OF_EPISODES})\n",
    "    env.reset()\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, done, info = env.last()\n",
    "        cumulative_reward[agent_mapping[agent]] += reward\n",
    "        agent_models[agent].save_memory(observation, reward, done)\n",
    "\n",
    "    #     Renders the environment for each step in a seperate window.\n",
    "#         if (get_current_step(env) % agent_models[agent].learning_steps) == 0:\n",
    "        agent_models[agent].replay()\n",
    "\n",
    "#             print(get_current_step(env) % agent_models[agent].learning_steps)\n",
    "#             prediction = agent_models[agent].predict(observation)\n",
    "#             if prediction is not None:\n",
    "#                 print(prediction)\n",
    "#                 print(np.argmax(prediction))\n",
    "            \n",
    "            \n",
    "#         env.render(mode='human')\n",
    "    #     Steps the environment forward.\n",
    "        if done:\n",
    "            env.step(None)\n",
    "            reward_array[episode,agent_mapping[agent]] = cumulative_reward[agent_mapping[agent]]\n",
    "        else:\n",
    "            action_to_take = agent_models[agent].policy(observation)\n",
    "            env.step(action_to_take)\n",
    "            agent_models[agent].save_action_state(action_to_take, observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2731d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closs the render window.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a286a",
   "metadata": {},
   "source": [
    "### Print Reward Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84502de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_no_e(reward_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461450ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620f5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4730628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
