{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6720f11",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7aa313",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To be able to run this notebook properly please make sure to install the pettingzoo package and dependencies. This can be done by running the following command\n",
    "\n",
    "`pip install pettingzoo[mpe]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc3cd2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9eecab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_world_comm_v2\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4441908",
   "metadata": {},
   "source": [
    "### Environment Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4dd63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CYCLES = 250\n",
    "NUM_OF_EPISODES = 10\n",
    "\n",
    "\n",
    "env = simple_world_comm_v2.env(num_good=2, num_adversaries=4, num_obstacles=1,\n",
    "                num_food=2, max_cycles=MAX_CYCLES, num_forests=2, continuous_actions=False)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082db82",
   "metadata": {},
   "source": [
    "### Policy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b66f4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(actions):\n",
    "    return random.randint(0, actions-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a679a49",
   "metadata": {},
   "source": [
    "### Running the environment\n",
    "\n",
    "The `env.render(mode='human')` call will pop open a new window that shows the environment at each time step.\n",
    "\n",
    "On my machine at least this window can only be closed while the cell is running but then freezes and is unable to be closed afterwards. In these cases restarting the kernel closed the window and any others which may have been opened due to running the cell multiple times.\n",
    "\n",
    "Eventually running the cell enough times without restarting the kernal will cause the render call to throw an exception and not run. In this case just restart the kernal and it will begin working again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ba70ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: ['leadadversary_0', 'adversary_0', 'adversary_1', 'adversary_2', 'agent_0', 'agent_1']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Agents: {env.agents}\")\n",
    "print()\n",
    "agent_mapping = {k: v for v, k in enumerate(env.agents)}\n",
    "reward_array = np.zeros((NUM_OF_EPISODES,len(env.agents)))\n",
    "cumulative_reward = np.zeros(len(env.agents))\n",
    "\n",
    "for episode in range(NUM_OF_EPISODES):\n",
    "    env.reset()\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, done, info = env.last()\n",
    "        cumulative_reward[agent_mapping[agent]] += reward\n",
    "        \n",
    "        \n",
    "    #     Various print statements which can help see what is returned at each step.\n",
    "    #     Commented out due to how verbose they are. Try toggling them on one at a time.\n",
    "#         print(f\"Current Agent: {env.agent_selection}\")\n",
    "    #     print(f\"Obs: {observation}\")\n",
    "#         print(f\"Rew: {reward}\")      \n",
    "    #     print(f\"Done: {done}\")\n",
    "    #     print(f\"Info: {info}\")\n",
    "\n",
    "    #     Renders the environment for each step in a seperate window.\n",
    "        env.render(mode='human')\n",
    "\n",
    "    #     Steps the environment forward.\n",
    "        if done:\n",
    "            env.step(None)\n",
    "            reward_array[episode,agent_mapping[agent]] = cumulative_reward[agent_mapping[agent]]\n",
    "        else:\n",
    "            env.step(random_policy(env.action_space(agent).n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a286a",
   "metadata": {},
   "source": [
    "### Print Reward Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84502de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.36388914e+00  7.41530256e+00 -7.29109056e+00 -1.52976672e+01\n",
      "  -1.57283010e+03 -3.29932329e+03]\n",
      " [-1.32408110e+01 -1.94953344e+01 -3.32330459e+01 -5.08807089e+01\n",
      "  -5.25807207e+03 -5.20731062e+03]\n",
      " [-7.55474508e+01 -6.08977836e+01 -6.50771838e+01 -9.59135860e+01\n",
      "  -8.95645338e+03 -7.28613425e+03]\n",
      " [-1.24291135e+02 -9.31254460e+01 -8.96201471e+01 -1.19633351e+02\n",
      "  -9.34300684e+03 -8.59137560e+03]\n",
      " [-1.31170180e+02 -9.54509942e+01 -9.65099216e+01 -1.22177482e+02\n",
      "  -1.27656268e+04 -8.94805662e+03]\n",
      " [-1.95002646e+02 -1.27302022e+02 -1.32037098e+02 -1.40077209e+02\n",
      "  -1.30886361e+04 -1.49005916e+04]\n",
      " [-2.29094366e+02 -1.51703790e+02 -1.53673958e+02 -1.47199030e+02\n",
      "  -1.43516085e+04 -1.49495488e+04]\n",
      " [-2.50047581e+02 -2.09885983e+02 -2.00458090e+02 -1.53417977e+02\n",
      "  -1.74972131e+04 -1.72914900e+04]\n",
      " [-2.55803701e+02 -2.12969261e+02 -1.82523156e+02 -1.35931579e+02\n",
      "  -1.82995853e+04 -1.76165589e+04]\n",
      " [-2.75444274e+02 -2.31780948e+02 -2.25544091e+02 -1.58941131e+02\n",
      "  -1.84031584e+04 -1.79616620e+04]]\n"
     ]
    }
   ],
   "source": [
    "print(reward_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461450ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
