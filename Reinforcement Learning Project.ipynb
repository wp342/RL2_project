{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6720f11",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7aa313",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "To be able to run this notebook properly please make sure to install the pettingzoo package and dependencies. This can be done by running the following command\n",
    "\n",
    "`pip install pettingzoo[mpe]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc3cd2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eecab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_world_comm_v2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4441908",
   "metadata": {},
   "source": [
    "### Environment Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CYCLES = 50\n",
    "# Keep as a multiple of 10\n",
    "NUM_OF_EPISODES = 500\n",
    "MEANING_OF_LIFE = 42\n",
    "ENVIRONMENT_NAME = \"simple_world_comm\"\n",
    "\n",
    "env = simple_world_comm_v2.env(num_good=2, num_adversaries=4, num_obstacles=1,\n",
    "                num_food=2, max_cycles=MAX_CYCLES, num_forests=2, continuous_actions=False)\n",
    "\n",
    "\n",
    "\n",
    "env.seed(seed=MEANING_OF_LIFE)\n",
    "env.reset()\n",
    "print(f\"Agents: {env.agents}\")\n",
    "print()\n",
    "agent_mapping = {k: v for v, k in enumerate(env.agents)}\n",
    "inv_agent_map = {v: k for k, v in agent_mapping.items()}\n",
    "NUM_OF_AGENTS = len(env.agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf58dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8082db82",
   "metadata": {},
   "source": [
    "### Policy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(actions):\n",
    "    return random.randint(0, actions-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e92ab",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Inspection Functions\n",
    "\n",
    "A collection of inspection functions to help minimise clutter in the training loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e51a0b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def matching_agent_only(agent, desired_agent, function, function_args):\n",
    "#     To use this function, pass the agent and desired agent as the first two arguments,\n",
    "#     then pass the function reference and the arguments for the function as a tuple for the final argument.\n",
    "    if agent == desired_agent:\n",
    "        function(*function_args)\n",
    "\n",
    "\n",
    "def print_agent_rewards(agent, reward):\n",
    "    print(f\"{agent}:{reward}\")\n",
    "    \n",
    "def print_agent_state(agent, observation):\n",
    "    print(f\"{agent}: {observation}\")\n",
    "    \n",
    "def print_iter_info(agent,observation,reward,done,info):\n",
    "    print(f\"Current Agent: {agent}\")\n",
    "    print(f\"Obs: {observation}\")\n",
    "    print(f\"Rew: {reward}\")      \n",
    "    print(f\"Done: {done}\")\n",
    "    print(f\"Info: {info}\")\n",
    "\n",
    "def get_current_step(env):\n",
    "    return env.env.env.steps\n",
    "\n",
    "def np_array_no_e(array):\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(array)\n",
    "    np.set_printoptions(suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a679a49",
   "metadata": {},
   "source": [
    "### Running the environment\n",
    "\n",
    "The `env.render(mode='human')` call will pop open a new window that shows the environment at each time step.\n",
    "\n",
    "On my machine at least this window can only be closed while the cell is running but then freezes and is unable to be closed afterwards. In these cases restarting the kernel closed the window and any others which may have been opened due to running the cell multiple times.\n",
    "\n",
    "Eventually running the cell enough times without restarting the kernal will cause the render call to throw an exception and not run. In this case just restart the kernal and it will begin working again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882818b3",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class agent_stub:\n",
    "    def __init__(self):\n",
    "        self.target_update_steps = 123456789123456789\n",
    "    \n",
    "    def policy(self, state):\n",
    "        return random_policy(env.action_space(agent).n)\n",
    "    \n",
    "    def save_action_state(self, action, state):\n",
    "        pass\n",
    "    \n",
    "    def save_memory(self, state, reward, done):\n",
    "        pass\n",
    "    \n",
    "    def replay(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, state):\n",
    "        pass\n",
    "\n",
    "    def update_target_network_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edeef41",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class good_agent_stub:\n",
    "    def __init__(self):\n",
    "        self.target_update_steps = 123456789123456789\n",
    "    \n",
    "    def policy(self, state):\n",
    "        return 0\n",
    "    \n",
    "    def save_action_state(self, action, state):\n",
    "        pass\n",
    "    \n",
    "    def save_memory(self, state, reward, done):\n",
    "        pass\n",
    "    \n",
    "    def replay(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, state):\n",
    "        pass\n",
    "    \n",
    "    def update_target_network_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f64d6d",
   "metadata": {
    "code_folding": [
     104
    ]
   },
   "outputs": [],
   "source": [
    "class agent:\n",
    "    \n",
    "    def __init__(self, agent_name, state_size, epsilon=1, epsilon_min = 0.1, \n",
    "                 epsilon_decay = 0.975, batch_size=16, learning_decay_rate = 0.95,\n",
    "                target_update_steps = 10, action_space=5, experience_replay = True):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_space = 5\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.agent_name = agent_name\n",
    "        self.history = []\n",
    "        self.action_taken = None\n",
    "        self.previous_state = None\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = learning_decay_rate\n",
    "        self.target_update_steps = target_update_steps\n",
    "        self.update_target_network_weights()\n",
    "        self.replay_enabled = experience_replay\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(self.state_size, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(32, activation=\"relu\"))\n",
    "        model.add(layers.Dense(32, activation=\"relu\"))\n",
    "        model.add(layers.Dense(32, activation=\"relu\"))\n",
    "        model.add(layers.Dense(self.action_space, activation=\"relu\"))\n",
    "        model.compile(loss=keras.losses.Huber(), \n",
    "                      optimizer = keras.optimizers.Adam(learning_rate=0.0005))\n",
    " \n",
    "        return model\n",
    "    \n",
    "    def exploration_decay(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = min(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "            \n",
    "    def save_model(self, agent_filename, target_filename):\n",
    "        self.model.save(agent_filename+\".h5\")\n",
    "        self.target_model.save(target_filename+\".h5\")\n",
    "    \n",
    "    def load_model(self, agent_filename, target_filename):\n",
    "        self.model.load_weights(agent_filename)\n",
    "        self.target_model.load_weights(target_filename)\n",
    "        \n",
    "    def policy(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        action_vals = self.predict(state)\n",
    "        return np.argmax(action_vals[0])\n",
    "    \n",
    "    def save_action_state(self, action, state):\n",
    "        self.action_taken = action\n",
    "        self.previous_state = state\n",
    "        \n",
    "    def save_history(self, state, reward, done):\n",
    "        if self.previous_state is None and self.action_taken is None:\n",
    "            self.previous_state = state\n",
    "            self.action_taken = 0\n",
    "        self.history.append((self.previous_state, self.action_taken, reward, state, done))\n",
    "    \n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "    \n",
    "    def replay(self):\n",
    "        if self.replay_enabled:\n",
    "            if len(self.history) < self.batch_size:\n",
    "                return\n",
    "            sample_batch = random.sample(self.history, self.batch_size)\n",
    "            targets = []\n",
    "            states = []\n",
    "\n",
    "            for state, action, reward, next_state, done in sample_batch:\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target = reward + self.gamma * np.amax(self.target_predict(next_state)[0])\n",
    "                target_f = self.predict(state)\n",
    "                target_f[0][action] = target\n",
    "                targets.append(target_f[0])\n",
    "                states.append(state)      \n",
    "            \n",
    "#             print(np.asarray(targets).shape)\n",
    "#             print(np.asarray(states).shape)\n",
    "            \n",
    "            self.model.fit(np.asarray(states),np.asarray(targets), epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        else:\n",
    "#             print(self.history[-1])\n",
    "            state, action, reward, next_state, done = self.history[-1]\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.target_predict(next_state)[0])\n",
    "            target_f = self.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            state = np.reshape(state, (1,self.state_size))\n",
    "            self.model.fit(state,target_f, epochs=1, verbose=0,)\n",
    "        \n",
    "        self.exploration_decay()\n",
    "    \n",
    "    def get_learning_steps(self):\n",
    "        return self.learning_steps\n",
    "    \n",
    "    def predict(self, state):\n",
    "#         print(f\"Before Reshape: {state}\")\n",
    "        state = np.reshape(state, (1,self.state_size))\n",
    "#         print(f\"After Reshape: {state}\")\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def target_predict(self, state):\n",
    "        state = np.reshape(state, (1,self.state_size))\n",
    "        return self.target_model.predict(state)\n",
    "\n",
    "    def update_target_network_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba70ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.seed(seed=MEANING_OF_LIFE)\n",
    "env.reset()\n",
    "reward_array = np.zeros((NUM_OF_EPISODES,len(env.agents)))\n",
    "\n",
    "cumulative_reward = np.zeros(len(env.agents))\n",
    "i = 0\n",
    "agent_models = {}\n",
    "\n",
    "ten_percent_episodes = int(NUM_OF_EPISODES/10)\n",
    "save_models = True\n",
    "\n",
    "\n",
    "# Creates the models. Models are shared between multiple adversaries and agents so for the first one of\n",
    "# each type it creates the models while subsequent agents get the reference to the model passed as their\n",
    "# 'model'\n",
    "for key in agent_mapping:\n",
    "    if key == \"leadadversary_0\":\n",
    "        agent_models[key] = agent(key, env.observation_space(key)._shape[0], \n",
    "                                  action_space=env.action_space(\"leadadversary_0\").n)\n",
    "    elif \"adversary\" in key:\n",
    "        if key == \"adversary_0\":    \n",
    "            agent_models[key] = agent(key, env.observation_space(key)._shape[0])\n",
    "        else:\n",
    "            agent_models[key] = agent_models[\"adversary_0\"]\n",
    "    elif \"agent\" in key:\n",
    "        if \"agent_0\" == key:\n",
    "            agent_models[key] = agent(key, env.observation_space(key)._shape[0])\n",
    "        else:\n",
    "            agent_models[key] = agent_models[\"agent_0\"]\n",
    "\n",
    "cycle_scores = np.zeros((len(env.agents), NUM_OF_EPISODES,MAX_CYCLES+1))\n",
    "step = -1\n",
    "episode_times = []  \n",
    "\n",
    "\n",
    "for episode in range(NUM_OF_EPISODES):\n",
    "    print(f\"Episode {episode+1} out of {NUM_OF_EPISODES}\")\n",
    "    env.seed(seed=MEANING_OF_LIFE)\n",
    "    env.reset()\n",
    "    cumulative_reward.fill(0)\n",
    "    if episode > 0:\n",
    "        print(f\"Time taken for previous episode: {time.time()-episode_start_time}\")\n",
    "    episode_start_time = time.time()\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        step_time = time.time()    \n",
    "        if step != get_current_step(env):\n",
    "#             print(f\"Step: {get_current_step(env)}/{MAX_CYCLES}\")\n",
    "            step = get_current_step(env)\n",
    "#         print(agent)\n",
    "        \n",
    "        observation, reward, done, info = env.last()\n",
    "        cumulative_reward[agent_mapping[agent]] += reward\n",
    "        cycle_scores[agent_mapping[agent]][episode][step] = reward\n",
    "        \n",
    "        agent_models[agent].save_history(observation, reward, done)\n",
    "        \n",
    "        if agent in [\"adversaryleader_0\", \"adversary_2\", \"agent_1\"]:\n",
    "            agent_models[agent].replay()\n",
    "            if (step % agent_models[agent].target_update_steps) == 0:\n",
    "                agent_models[agent].update_target_network_weights()\n",
    "\n",
    "            if (episode % ten_percent_episodes == 0) and save_models:\n",
    "                agent_models[agent].save_model(\n",
    "                    ENVIRONMENT_NAME+\"_\"+agent+\"_\"+str(episode)+\"_of_\"+str(NUM_OF_EPISODES)+\"_model\",\n",
    "                ENVIRONMENT_NAME+\"_\"+agent+\"_\"+str(episode)+\"_of_\"+str(NUM_OF_EPISODES)+\"_target_model\")\n",
    "\n",
    "            \n",
    "    #     Renders the environment for each step in a seperate window.        \n",
    "#         env.render(mode='human')\n",
    "    \n",
    "    \n",
    "    #     Steps the environment forward.\n",
    "        if done:\n",
    "            env.step(None)\n",
    "            reward_array[episode,agent_mapping[agent]] = cumulative_reward[agent_mapping[agent]]\n",
    "        else:\n",
    "            action_to_take = agent_models[agent].policy(observation)\n",
    "            env.step(action_to_take)\n",
    "            agent_models[agent].save_action_state(action_to_take, observation)\n",
    "        \n",
    "#         print(f\"Step Time: {time.time()-step_time}\")\n",
    "   \n",
    "\n",
    "    episode_time = time.time()-episode_start_time\n",
    "    episode_times.append(episode_time)\n",
    "    print(f\"Episode Time: {episode_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closs the render window.\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a286a",
   "metadata": {},
   "source": [
    "### Print Reward Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84502de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_no_e(reward_array)\n",
    "\n",
    "x = 1\n",
    "\n",
    "with open(f\"{ENVIRONMENT_NAME}_DQN_{NUM_OF_EPISODES}ep.csv\", \"w+\") as f:\n",
    "    f.write(\"episode,adversary_0,agent_0\\n\")\n",
    "    for j, i in enumerate(reward_array):\n",
    "        f.write(f\"{x}, {i[0]},{i[1]}, {i[2]},{i[3]},{i[4]},{i[5]}\\n\")\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_array_no_e(cycle_scores[0])\n",
    "# print()\n",
    "# print()\n",
    "# print()\n",
    "# np_array_no_e(cycle_scores[1])\n",
    "# np_array_no_e(cycle_scores[1].sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461450ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(datetime.timedelta(seconds=np.mean(episode_times))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71b9ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,NUM_OF_AGENTS):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.plot(range(1,NUM_OF_EPISODES+1), reward_array[:,i])\n",
    "    plt.title(inv_agent_map[i])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.savefig(f\"{ENVIRONMENT_NAME}_{inv_agent_map[i]}_learning_\"+str(NUM_OF_EPISODES)+\"_episodes.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c80fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd2b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(vars(env.env.env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de90855",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_models[\"adversary_0\"].save_model(\n",
    "    ENVIRONMENT_NAME+\"_adversary_0_\"+str(NUM_OF_EPISODES)+\"_ep_final_model\", \n",
    "    ENVIRONMENT_NAME+\"_adversary_0_\"+str(NUM_OF_EPISODES)+\"_final_target_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc75dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_models[\"agent_0\"].save_model(\n",
    "    ENVIRONMENT_NAME+\"_agent_0_\"+str(NUM_OF_EPISODES)+\"_final_model\", \n",
    "    ENVIRONMENT_NAME+\"_agent_0_\"+str(NUM_OF_EPISODES)+\"_final_target_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f1ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
