{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f69c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_world_comm_v2, simple_push_v2\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pprint import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5220ac6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: ['leadadversary_0', 'adversary_0', 'adversary_1', 'adversary_2', 'agent_0', 'agent_1']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_CYCLES = 50\n",
    "# Keep as a multiple of 10\n",
    "NUM_OF_EPISODES = 1\n",
    "MEANING_OF_LIFE = 42\n",
    "ENVIRONMENT_NAME = \"simple_world_comm\"\n",
    "\n",
    "env = simple_world_comm_v2.env(num_good=2, num_adversaries=4, num_obstacles=1,\n",
    "                num_food=2, max_cycles=MAX_CYCLES, num_forests=2, continuous_actions=False)\n",
    "\n",
    "\n",
    "\n",
    "env.seed(seed=MEANING_OF_LIFE)\n",
    "env.reset()\n",
    "print(f\"Agents: {env.agents}\")\n",
    "print()\n",
    "agent_mapping = {k: v for v, k in enumerate(env.agents)}\n",
    "inv_agent_map = {v: k for k, v in agent_mapping.items()}\n",
    "NUM_OF_AGENTS = len(env.agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0efb7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(actions):\n",
    "    return random.randint(0, actions-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01cf5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    \n",
    "    def __init__(self, agent_name, state_size, epsilon=1, epsilon_min = 0.1, \n",
    "                 epsilon_decay = 0.975, batch_size=16, learning_decay_rate = 0.95,\n",
    "                target_update_steps = 10, action_space=5, experience_replay = True):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_space = 5\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.agent_name = agent_name\n",
    "        self.history = []\n",
    "        self.action_taken = None\n",
    "        self.previous_state = None\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = learning_decay_rate\n",
    "        self.target_update_steps = target_update_steps\n",
    "        self.update_target_network_weights()\n",
    "        self.replay_enabled = experience_replay\n",
    "    \n",
    "    def build_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Dense(self.state_size, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(32, activation=\"relu\"))\n",
    "        model.add(layers.Dense(32, activation=\"relu\"))\n",
    "        model.add(layers.Dense(32, activation=\"relu\"))\n",
    "        model.add(layers.Dense(self.action_space, activation=\"relu\"))\n",
    "        model.compile(loss=keras.losses.Huber(), \n",
    "                      optimizer = keras.optimizers.Adam(learning_rate=0.00025))\n",
    " \n",
    "        return model\n",
    "    \n",
    "    def exploration_decay(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = min(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "            \n",
    "    def save_model(self, agent_filename, target_filename):\n",
    "        self.model.save(agent_filename+\".h5\")\n",
    "        self.target_model.save(target_filename+\".h5\")\n",
    "    \n",
    "    def load_model(self, agent_filename, target_filename):\n",
    "        self.model.load_weights(agent_filename)\n",
    "        self.target_model.load_weights(target_filename)\n",
    "        \n",
    "    def policy(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_space)\n",
    "        action_vals = self.predict(state)\n",
    "        return np.argmax(action_vals[0])\n",
    "    \n",
    "    def save_action_state(self, action, state):\n",
    "        self.action_taken = action\n",
    "        self.previous_state = state\n",
    "        \n",
    "    def save_history(self, state, reward, done):\n",
    "        if self.previous_state is None and self.action_taken is None:\n",
    "            self.previous_state = state\n",
    "            self.action_taken = 0\n",
    "        self.history.append((self.previous_state, self.action_taken, reward, state, done))\n",
    "    \n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "    \n",
    "    def replay(self):\n",
    "        if self.replay_enabled:\n",
    "            if len(self.history) < self.batch_size:\n",
    "                return\n",
    "            sample_batch = random.sample(self.history, self.batch_size)\n",
    "            targets = []\n",
    "            states = []\n",
    "\n",
    "            for state, action, reward, next_state, done in sample_batch:\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target = reward + self.gamma * np.amax(self.target_predict(next_state)[0])\n",
    "                target_f = self.predict(state)\n",
    "                target_f[0][action] = target\n",
    "                targets.append(target_f[0])\n",
    "                states.append(state)      \n",
    "            \n",
    "#             print(np.asarray(targets).shape)\n",
    "#             print(np.asarray(states).shape)\n",
    "            \n",
    "            self.model.fit(np.asarray(states),np.asarray(targets), epochs=1, verbose=0, batch_size=self.batch_size)\n",
    "        else:\n",
    "#             print(self.history[-1])\n",
    "            state, action, reward, next_state, done = self.history[-1]\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.target_predict(next_state)[0])\n",
    "            target_f = self.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            state = np.reshape(state, (1,self.state_size))\n",
    "            self.model.fit(state,target_f, epochs=1, verbose=0,)\n",
    "        \n",
    "        self.exploration_decay()\n",
    "    \n",
    "    def get_learning_steps(self):\n",
    "        return self.learning_steps\n",
    "    \n",
    "    def predict(self, state):\n",
    "#         print(f\"Before Reshape: {state}\")\n",
    "        state = np.reshape(state, (1,self.state_size))\n",
    "#         print(f\"After Reshape: {state}\")\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def target_predict(self, state):\n",
    "        state = np.reshape(state, (1,self.state_size))\n",
    "        return self.target_model.predict(state)\n",
    "\n",
    "    def update_target_network_weights(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f802bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def play_episode(adversary_leader_model, adversary_leader_target_model, \n",
    "                 adversary_model, adversary_target_model,\n",
    "                 agent_model, agent_target_model):\n",
    "\n",
    "    env.seed(seed=MEANING_OF_LIFE)\n",
    "    env.reset()\n",
    "    reward_array = np.zeros((NUM_OF_EPISODES,len(env.agents)))\n",
    "\n",
    "    cumulative_reward = np.zeros(len(env.agents))\n",
    "    i = 0\n",
    "    agent_models = {}\n",
    "\n",
    "    ten_percent_episodes = int(NUM_OF_EPISODES/10)\n",
    "    save_models = True\n",
    "\n",
    "\n",
    "    # Creates the models. Models are shared between multiple adversaries and agents so for the first one of\n",
    "    # each type it creates the models while subsequent agents get the reference to the model passed as their\n",
    "    # 'model'\n",
    "    for key in agent_mapping:\n",
    "        if key == \"leadadversary_0\":\n",
    "            agent_models[key] = agent(key, env.observation_space(key)._shape[0], \n",
    "                                      action_space=env.action_space(\"leadadversary_0\").n)\n",
    "            agent_models[key].load_model(adversary_leader_model, adversary_leader_target_model)\n",
    "        elif \"adversary\" in key:\n",
    "            if key == \"adversary_0\":    \n",
    "                agent_models[key] = agent(key, env.observation_space(key)._shape[0])\n",
    "                agent_models[key].load_model(adversary_model, adversary_target_model)\n",
    "            else:\n",
    "                agent_models[key] = agent_models[\"adversary_0\"]\n",
    "        elif \"agent\" in key:\n",
    "            if \"agent_0\" == key:\n",
    "                agent_models[key] = agent(key, env.observation_space(key)._shape[0])\n",
    "                agent_models[key].load_model(agent_model, agent_target_model)\n",
    "            else:\n",
    "                agent_models[key] = agent_models[\"agent_0\"]\n",
    "\n",
    "\n",
    "    step = -1\n",
    "    episode_times = []  \n",
    "\n",
    "    for episode in range(NUM_OF_EPISODES):\n",
    "        env.seed(seed=MEANING_OF_LIFE)\n",
    "        env.reset()\n",
    "\n",
    "        for agent in env.agent_iter(): \n",
    "            observation, reward, done, info = env.last()\n",
    "        #     Renders the environment for each step in a seperate window.        \n",
    "            env.render(mode='human')\n",
    "\n",
    "        #     Steps the environment forward.\n",
    "            if done:\n",
    "                env.step(None)\n",
    "            else:\n",
    "                action_to_take = agent_models[agent].policy(observation)\n",
    "                env.step(action_to_take)\n",
    "\n",
    "    #         print(f\"Step Time: {time.time()-step_time}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model = \"models/simple_world_comm/simple_world_comm_adversary_2_0_of_500_model.h5\"\n",
    "adversary_target_model = \"models/simple_world_comm/simple_world_comm_adversary_2_0_of_500_target_model.h5\"\n",
    "adversary_leader_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_0_of_500_model.h5\"\n",
    "adversary_leader_target_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_0_of_500_target_model.h5\"\n",
    "agent_model = \"models/simple_world_comm/simple_world_comm_agent_1_0_of_500_model.h5\"\n",
    "agent_target_model = \"models/simple_world_comm/simple_world_comm_agent_1_0_of_500_target_model.h5\"\n",
    "\n",
    "play_episode(adversary_leader_model, adversary_leader_target_model, \n",
    "                 adversary_model, adversary_target_model,\n",
    "                 agent_model, agent_target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e55682",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model = \"models/simple_world_comm/simple_world_comm_adversary_2_250_of_500_model.h5\"\n",
    "adversary_target_model = \"models/simple_world_comm/simple_world_comm_adversary_2_250_of_500_target_model.h5\"\n",
    "adversary_leader_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_250_of_500_model.h5\"\n",
    "adversary_leader_target_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_250_of_500_target_model.h5\"\n",
    "agent_model = \"models/simple_world_comm/simple_world_comm_agent_1_250_of_500_model.h5\"\n",
    "agent_target_model = \"models/simple_world_comm/simple_world_comm_agent_1_250_of_500_target_model.h5\"\n",
    "\n",
    "play_episode(adversary_leader_model, adversary_leader_target_model, \n",
    "                 adversary_model, adversary_target_model,\n",
    "                 agent_model, agent_target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model = \"models/simple_world_comm/simple_world_comm_adversary_2_500_final_model.h5\"\n",
    "adversary_target_model = \"models/simple_world_comm/simple_world_comm_adversary_2_500_final_target_model.h5\"\n",
    "adversary_leader_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_500_final_model.h5\"\n",
    "adversary_leader_target_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_500_final_target_model.h5\"\n",
    "agent_model = \"models/simple_world_comm/simple_world_comm_agent_1_500_final_model.h5\"\n",
    "agent_target_model = \"models/simple_world_comm/simple_world_comm_agent_1_500_final_target_model.h5\"\n",
    "\n",
    "play_episode(adversary_leader_model, adversary_leader_target_model, \n",
    "                 adversary_model, adversary_target_model,\n",
    "                 agent_model, agent_target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d47f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_CYCLES = 50\n",
    "# Keep as a multiple of 10\n",
    "NUM_OF_EPISODES = 100\n",
    "MEANING_OF_LIFE = 42\n",
    "ENVIRONMENT_NAME = \"simple_push\"\n",
    "\n",
    "env = simple_push_v2.env(max_cycles=MAX_CYCLES, continuous_actions=False)\n",
    "\n",
    "\n",
    "env.seed(seed=MEANING_OF_LIFE)\n",
    "env.reset()\n",
    "print(f\"Agents: {env.agents}\")\n",
    "print()\n",
    "agent_mapping = {k: v for v, k in enumerate(env.agents)}\n",
    "inv_agent_map = {v: k for k, v in agent_mapping.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model = \"models/simple_world_comm/simple_world_comm_adversary_2_500_final_model.h5\"\n",
    "adversary_target_model = \"models/simple_world_comm/simple_world_comm_adversary_2_500_final_target_model.h5\"\n",
    "adversary_leader_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_500_final_model.h5\"\n",
    "adversary_leader_target_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_500_final_target_model.h5\"\n",
    "agent_model = \"models/simple_world_comm/simple_world_comm_agent_1_500_final_model.h5\"\n",
    "agent_target_model = \"models/simple_world_comm/simple_world_comm_agent_1_500_final_target_model.h5\"\n",
    "\n",
    "play_episode(adversary_leader_model, adversary_leader_target_model, \n",
    "                 adversary_model, adversary_target_model,\n",
    "                 agent_model, agent_target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18de03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_model = \"models/simple_world_comm/simple_world_comm_adversary_2_500_final_model.h5\"\n",
    "adversary_target_model = \"models/simple_world_comm/simple_world_comm_adversary_2_500_final_target_model.h5\"\n",
    "adversary_leader_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_500_final_model.h5\"\n",
    "adversary_leader_target_model = \"models/simple_world_comm/simple_world_comm_adversaryleader_0_500_final_target_model.h5\"\n",
    "agent_model = \"models/simple_world_comm/simple_world_comm_agent_1_500_final_model.h5\"\n",
    "agent_target_model = \"models/simple_world_comm/simple_world_comm_agent_1_500_final_target_model.h5\"\n",
    "\n",
    "play_episode(adversary_leader_model, adversary_leader_target_model, \n",
    "                 adversary_model, adversary_target_model,\n",
    "                 agent_model, agent_target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87be3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e9451",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f3c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
